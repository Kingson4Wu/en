<!DOCTYPE html><html lang="en" data-astro-cid-bvzihdzo> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="https://kingson4wu.github.io/favicon.svg"><link rel="sitemap" href="https://kingson4wu.github.io/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Kingson Wu's Technical Blog" href="https://kingson4wu.github.io/rss.xml"><meta name="generator" content="Astro v5.16.5"><!-- Font preloads --><link rel="preload" href="https://kingson4wu.github.io/en/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="https://kingson4wu.github.io/en/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://kingson4wu.github.io/en/blog/nondeterminism-in-llm-inference-from-floating-point-arithmetic-to-engineering-implementation/"><!-- Primary Meta Tags --><title>Nondeterminism in LLM Inference: From Floating-Point Arithmetic to Engineering Implementation</title><meta name="title" content="Nondeterminism in LLM Inference: From Floating-Point Arithmetic to Engineering Implementation"><meta name="description" content="Why LLM inference produces different outputs with temperature=0: exploring how batch-variant operators in inference engines introduce nondeterminism through floating-point arithmetic, and how batch-invariant implementations achieve bitwise reproducibility for reinforcement learning."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://kingson4wu.github.io/en/blog/nondeterminism-in-llm-inference-from-floating-point-arithmetic-to-engineering-implementation/"><meta property="og:title" content="Nondeterminism in LLM Inference: From Floating-Point Arithmetic to Engineering Implementation"><meta property="og:description" content="Why LLM inference produces different outputs with temperature=0: exploring how batch-variant operators in inference engines introduce nondeterminism through floating-point arithmetic, and how batch-invariant implementations achieve bitwise reproducibility for reinforcement learning."><meta property="og:image" content="https://kingson4wu.github.io/en/_astro/blog-placeholder-1.Bx0Zcyzv.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://kingson4wu.github.io/en/blog/nondeterminism-in-llm-inference-from-floating-point-arithmetic-to-engineering-implementation/"><meta property="twitter:title" content="Nondeterminism in LLM Inference: From Floating-Point Arithmetic to Engineering Implementation"><meta property="twitter:description" content="Why LLM inference produces different outputs with temperature=0: exploring how batch-variant operators in inference engines introduce nondeterminism through floating-point arithmetic, and how batch-invariant implementations achieve bitwise reproducibility for reinforcement learning."><meta property="twitter:image" content="https://kingson4wu.github.io/en/_astro/blog-placeholder-1.Bx0Zcyzv.jpg"><style>:root{--accent: #2337ff;--accent-dark: #000d8a;--black: 15, 18, 25;--gray: 96, 115, 159;--gray-light: 229, 233, 240;--gray-dark: 34, 41, 57;--gray-gradient: rgba(var(--gray-light), 50%), #fff;--box-shadow: 0 2px 6px rgba(var(--gray), 25%), 0 8px 24px rgba(var(--gray), 33%), 0 16px 32px rgba(var(--gray), 33%)}@font-face{font-family:Atkinson;src:url(/en/fonts/atkinson-regular.woff) format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/en/fonts/atkinson-bold.woff) format("woff");font-weight:700;font-style:normal;font-display:swap}body{font-family:Atkinson,sans-serif;margin:0;padding:0;text-align:left;background:linear-gradient(var(--gray-gradient)) no-repeat;background-size:100% 600px;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));font-size:20px;line-height:1.7}main{width:720px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{margin:0 0 .5rem;color:rgb(var(--black));line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}.prose p{margin-bottom:2em}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%}img{max-width:100%;height:auto;border-radius:8px}code{padding:2px 5px;background-color:rgb(var(--gray-light));border-radius:2px}pre{padding:1.5em;border-radius:8px}pre>code{all:unset}blockquote{border-left:4px solid var(--accent);padding:0 0 0 20px;margin:0;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}@media(max-width:720px){body{font-size:18px}main{padding:1em}}.sr-only{border:0;padding:0;margin:0;position:absolute!important;height:1px;width:1px;overflow:hidden;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media(max-width:720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}
main[data-astro-cid-bvzihdzo]{width:calc(100% - 2em);max-width:100%;margin:0}.hero-image[data-astro-cid-bvzihdzo]{width:100%}.hero-image[data-astro-cid-bvzihdzo] img[data-astro-cid-bvzihdzo]{display:block;margin:0 auto;border-radius:12px;box-shadow:var(--box-shadow)}.prose[data-astro-cid-bvzihdzo]{width:720px;max-width:calc(100% - 2em);margin:auto;padding:1em;color:rgb(var(--gray-dark))}.title[data-astro-cid-bvzihdzo]{margin-bottom:1em;padding:1em 0;text-align:center;line-height:1}.title[data-astro-cid-bvzihdzo] h1[data-astro-cid-bvzihdzo]{margin:0 0 .5em}.date[data-astro-cid-bvzihdzo]{margin-bottom:.5em;color:rgb(var(--gray))}.last-updated-on[data-astro-cid-bvzihdzo]{font-style:italic}
a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <h2 data-astro-cid-3ef6ksr2><a href="/en/" data-astro-cid-3ef6ksr2>Kingson Wu&#39;s Technical Blog</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/en/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/en/blog" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Blog </a>  <a href="/en/about" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> About </a>  </div> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://m.webtoo.ls/@astro" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Follow Astro on Mastodon</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M11.19 12.195c2.016-.24 3.77-1.475 3.99-2.603.348-1.778.32-4.339.32-4.339 0-3.47-2.286-4.488-2.286-4.488C12.062.238 10.083.017 8.027 0h-.05C5.92.017 3.942.238 2.79.765c0 0-2.285 1.017-2.285 4.488l-.002.662c-.004.64-.007 1.35.011 2.091.083 3.394.626 6.74 3.78 7.57 1.454.383 2.703.463 3.709.408 1.823-.1 2.847-.647 2.847-.647l-.06-1.317s-1.303.41-2.767.36c-1.45-.05-2.98-.156-3.215-1.928a3.614 3.614 0 0 1-.033-.496s1.424.346 3.228.428c1.103.05 2.137-.064 3.188-.189zm1.613-2.47H11.13v-4.08c0-.859-.364-1.295-1.091-1.295-.804 0-1.207.517-1.207 1.541v2.233H7.168V5.89c0-1.024-.403-1.541-1.207-1.541-.727 0-1.091.436-1.091 1.296v4.079H3.197V5.522c0-.859.22-1.541.66-2.046.456-.505 1.052-.764 1.793-.764.856 0 1.504.328 1.933.983L8 4.39l.417-.695c.429-.655 1.077-.983 1.934-.983.74 0 1.336.259 1.791.764.442.505.661 1.187.661 2.046v4.203z" data-astro-cid-3ef6ksr2></path></svg> </a> <a href="https://twitter.com/astrodotbuild" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Follow Astro on Twitter</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334 0-.14 0-.282-.006-.422A6.685 6.685 0 0 0 16 3.542a6.658 6.658 0 0 1-1.889.518 3.301 3.301 0 0 0 1.447-1.817 6.533 6.533 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.325 9.325 0 0 1-6.767-3.429 3.289 3.289 0 0 0 1.018 4.382A3.323 3.323 0 0 1 .64 6.575v.045a3.288 3.288 0 0 0 2.632 3.218 3.203 3.203 0 0 1-.865.115 3.23 3.23 0 0 1-.614-.057 3.283 3.283 0 0 0 3.067 2.277A6.588 6.588 0 0 1 .78 13.58a6.32 6.32 0 0 1-.78-.045A9.344 9.344 0 0 0 5.026 15z" data-astro-cid-3ef6ksr2></path></svg> </a> <a href="https://github.com/withastro/astro" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Go to Astro's GitHub repo</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path></svg> </a> </div> </nav> </header>  <main data-astro-cid-bvzihdzo> <article data-astro-cid-bvzihdzo> <div class="hero-image" data-astro-cid-bvzihdzo>  </div> <div class="prose" data-astro-cid-bvzihdzo> <div class="title" data-astro-cid-bvzihdzo> <div class="date" data-astro-cid-bvzihdzo> <time datetime="2025-12-16T16:00:00.000Z"> Dec 17, 2025 </time> <div class="last-updated-on" data-astro-cid-bvzihdzo>
Last updated on <time datetime="2025-12-16T16:00:00.000Z"> Dec 17, 2025 </time> </div> </div> <h1 data-astro-cid-bvzihdzo>Nondeterminism in LLM Inference: From Floating-Point Arithmetic to Engineering Implementation</h1> <hr data-astro-cid-bvzihdzo> </div>  <h2 id="the-problem">The Problem</h2>
<p>Why do large language models produce different outputs for identical inputs even when <code>temperature=0</code>? This counterintuitive phenomenon reveals the engineering trade-offs modern inference engines make in pursuit of extreme performance.</p>
<h2 id="the-root-cause">The Root Cause</h2>
<h3 id="non-associativity-of-floating-point-operations">Non-Associativity of Floating-Point Operations</h3>
<p>Floating-point arithmetic in computers does not satisfy the associative property. While mathematically <code>(a + b) + c = a + (b + c)</code> always holds, this equality may fail with finite-precision floating-point operations due to rounding errors.</p>
<p><strong>Concrete Example</strong>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Numbers: x=10000000, y=1, z=-10000000</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Order 1: (x + y) + z</span></span>
<span class="line"><span>       = 10000001 + z     // Precision loss, becomes 10000000</span></span>
<span class="line"><span>       = 0</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Order 2: x + (y + z)  </span></span>
<span class="line"><span>       = x + (-9999999)</span></span>
<span class="line"><span>       = 1</span></span></code></pre>
<h3 id="parallel-computation-alters-operation-order">Parallel Computation Alters Operation Order</h3>
<p>To maximize efficiency, GPU parallel computation splits sequential calculations into multiple parallel paths and then merges results. Different parallelization strategies mean different addition tree structures, leading to different floating-point rounding paths.</p>
<p><strong>Serial Computation</strong>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#79B8FF">sum</span><span style="color:#F97583"> =</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> data:</span></span>
<span class="line"><span style="color:#79B8FF">    sum</span><span style="color:#F97583"> +=</span><span style="color:#E1E4E8"> i  </span><span style="color:#6A737D"># Fixed order</span></span></code></pre>
<p><strong>Parallel Computation (2 threads)</strong>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Thread 1: (((a+b)+c)+d)</span></span>
<span class="line"><span>Thread 2: (((e+f)+g)+h)</span></span>
<span class="line"><span>Final: thread1 + thread2</span></span></code></pre>
<p><strong>Parallel Computation (4 threads)</strong>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>t1: a+b,  t2: c+d,  t3: e+f,  t4: g+h</span></span>
<span class="line"><span>Then: (t1+t2) + (t3+t4)</span></span></code></pre>
<p>While mathematically equivalent, the topology of the addition tree is completely different, resulting in different floating-point accumulation errors.</p>
<h2 id="the-batch-variant-problem">The Batch-Variant Problem</h2>
<h3 id="dynamic-optimization-in-inference-engines">Dynamic Optimization in Inference Engines</h3>
<p>Modern inference engines (e.g., vLLM, TensorRT) dynamically select parallelization strategies based on current load to achieve maximum GPU utilization:</p>





















<table><thead><tr><th>Batch Size</th><th>Parallelization Strategy</th></tr></thead><tbody><tr><td>Small batches</td><td>Simple kernels</td></tr><tr><td>Large batches</td><td>Complex parallel kernels</td></tr><tr><td>Mixed workloads</td><td>Dynamic kernel switching</td></tr></tbody></table>
<p>This means <strong>the same input takes different computational paths under different loads</strong>.</p>
<h3 id="batch-variant-characteristics-of-key-operators">Batch-Variant Characteristics of Key Operators</h3>
<p>Three operators most prone to nondeterminism:</p>
<ol>
<li><strong>RMSNorm</strong>: Requires reduction across hidden dimensions; reduction tree structure varies with batch size</li>
<li><strong>MatMul</strong>: Large-scale matrix multiplication accumulation order is highly sensitive</li>
<li><strong>Attention</strong>: The exp-sum-normalize chain in softmax is a hotspot for numerical instability</li>
</ol>
<h2 id="argmax-the-amplifier-of-tiny-errors">argmax: The Amplifier of Tiny Errors</h2>
<h3 id="what-is-argmax">What is argmax</h3>
<p>argmax returns not the maximum value itself, but <strong>the position of the maximum value</strong>.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">logits </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span><span style="color:#79B8FF">5.000000</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">4.999999</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">3.2</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">argmax(logits) </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span><span style="color:#6A737D">  # Returns token at index 0</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># But if parallel path changes cause tiny errors</span></span>
<span class="line"><span style="color:#E1E4E8">logits </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span><span style="color:#79B8FF">4.999998</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">4.999999</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">3.2</span><span style="color:#E1E4E8">]  </span></span>
<span class="line"><span style="color:#E1E4E8">argmax(logits) </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 1</span><span style="color:#6A737D">  # Returns token at index 1</span></span></code></pre>
<h3 id="why-so-fragile">Why So Fragile</h3>
<p>argmax is a <strong>cliff-edge mapping from continuous to discrete</strong>:</p>
<ul>
<li>Before argmax: numerical changes are smooth</li>
<li>After argmax: results are binary (black or white)</li>
</ul>
<p>Therefore, a 0.000001 numerical error can lead to:</p>
<ul>
<li>100% different token selection</li>
<li>Completely different subsequent generation paths</li>
<li>Total divergence of generated text</li>
</ul>
<p>This is why <code>temperature=0</code> is actually the most unstable—it relies entirely on the fragile “blade” of argmax.</p>
<h2 id="the-solution-batch-invariant-operators">The Solution: Batch-Invariant Operators</h2>
<h3 id="core-idea">Core Idea</h3>
<p>Not eliminating parallelism, but <strong>keeping the reduction structure consistent across any batch size</strong>.</p>
<h3 id="implementation-approach">Implementation Approach</h3>
<ol>
<li><strong>Fixed reduction tree</strong>: Use the same addition tree regardless of batch size</li>
<li><strong>Disable automatic kernel switching</strong>: Explicitly specify computation paths; prevent engine from dynamically selecting based on load</li>
<li><strong>Unified normalization order</strong>: Force fixed computation order in attention and softmax</li>
</ol>
<h3 id="trade-offs">Trade-offs</h3>
<ul>
<li>✅ Achieves complete determinism (bitwise identical results)</li>
<li>❌ Sacrifices some GPU throughput and dynamic optimization capability</li>
</ul>
<h3 id="experimental-validation">Experimental Validation</h3>
<p>On Qwen3-235B model:</p>
<ul>
<li><strong>Before fix</strong>: Same prompt produces 80 different outputs across 1000 inferences</li>
<li><strong>After fix</strong>: 1000 inferences produce identical output</li>
</ul>
<h2 id="critical-impact-on-reinforcement-learning">Critical Impact on Reinforcement Learning</h2>
<h3 id="on-policy-vs-off-policy">On-Policy vs Off-Policy</h3>
<p>In reinforcement learning, on-policy requires:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Sampling policy π_sample = Training assumed policy π_train</span></span></code></pre>
<p>But due to inference nondeterminism:</p>
<ul>
<li>You think you’re doing greedy sampling (<code>temperature=0</code>)</li>
<li>Actually argmax boundaries keep flipping</li>
<li>Resulting in <code>π_sample ≠ π_train</code></li>
<li>Becomes <strong>pseudo off-policy</strong></li>
</ul>
<h3 id="kl-divergence-verification">KL Divergence Verification</h3>
<p>After adopting batch-invariant operators, KL divergence during training remains at 0, proving complete consistency between sampling and training. This is nearly impossible in traditional LLM reinforcement learning.</p>
<h2 id="engineering-status-and-outlook">Engineering Status and Outlook</h2>
<h3 id="current-state">Current State</h3>
<ul>
<li>✅ Working research prototype available (<a href="https://github.com/thinking-machines-lab/batch_invariant_ops">GitHub repository</a>)</li>
<li>✅ Validated on 235B-scale models</li>
<li>❌ Not yet integrated into mainstream inference engines (vLLM, TensorRT)</li>
</ul>
<h3 id="why-not-yet-adopted">Why Not Yet Adopted</h3>
<ol>
<li><strong>Performance cost</strong>: Fixed computation paths mean abandoning dynamic optimization</li>
<li><strong>Priority mismatch</strong>: Most applications use <code>temperature>0</code>, which already allows randomness</li>
<li><strong>Design philosophy conflict</strong>: Mainstream engines prioritize throughput over determinism</li>
</ol>
<h3 id="understanding-the-scope-of-the-solution">Understanding the Scope of the Solution</h3>
<p>This approach is easily misunderstood as a “permanent reproducibility” solution, but it actually addresses <strong>local temporal consistency</strong>.</p>
<p><strong>What it does NOT guarantee</strong>:</p>
<ul>
<li>Cross-version reproducibility (model weights, tokenizers will update)</li>
<li>Cross-time reproducibility (inference engines, CUDA versions will change)</li>
<li>Historical archival replay (doesn’t record kernel versions, reduction trees)</li>
</ul>
<p><strong>What it truly guarantees</strong>:</p>
<ul>
<li>Within the same model version, same inference system, same deployment cycle</li>
<li>Inference results do not drift due to load and scheduling variations</li>
<li>This is about “eliminating system noise,” not “freezing history”</li>
</ul>
<p>By analogy, this is more like <strong>database transaction isolation levels</strong> rather than permanent snapshots—it guarantees consistent behavior within a transaction, but not replay of the same transaction ten years later.</p>
<p>Why not record the complete computation path? Because recording every kernel, every block/warp, every floating-point rounding point on a 235B model is infeasible in terms of storage, replay, and performance. The approach chosen is <strong>structural constraints to guarantee path equivalence</strong>—the only engineering-viable route.</p>
<h3 id="true-application-scenarios">True Application Scenarios</h3>
<p>The core value of this solution lies in <strong>consistency within the same time window</strong>:</p>
<ol>
<li>
<p><strong>Reinforcement Learning Training</strong>: In a single training round, if the sampling policy drifts due to batch changes, that round is already contaminated. This isn’t about whether we can reproduce results three months later—it’s about maintaining on-policy within the current training cycle.</p>
</li>
<li>
<p><strong>Scientific Research</strong>: Requires bitwise-level reproducibility during the experiment period to eliminate system noise from interfering with experimental conclusions.</p>
</li>
<li>
<p><strong>Security Auditing</strong>: Within the audit period, identical inputs must produce identical outputs to support behavior tracing.</p>
</li>
</ol>
<h3 id="future-form">Future Form</h3>
<p>More likely to appear as an <strong>optional mode</strong> in inference engines:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">vllm</span><span style="color:#9ECBFF"> serve</span><span style="color:#79B8FF"> --deterministic</span></span>
<span class="line"><span style="color:#B392F0">vllm</span><span style="color:#9ECBFF"> serve</span><span style="color:#79B8FF"> --batch-invariant</span></span>
<span class="line"><span style="color:#B392F0">vllm</span><span style="color:#9ECBFF"> serve</span><span style="color:#79B8FF"> --rl-training-mode</span></span></code></pre>
<p>Similar to PyTorch’s <code>torch.use_deterministic_algorithms(True)</code>, allowing users to choose between performance and determinism.</p>
<h2 id="temperature-and-randomness">Temperature and Randomness</h2>
<h3 id="the-role-of-temperature">The Role of Temperature</h3>
<p>Temperature doesn’t directly control “whether it’s random” but rather <strong>adjusts the steepness of the probability distribution</strong>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>p_i = exp(z_i / T) / Σ exp(z_j / T)</span></span></code></pre>






























<table><thead><tr><th>Temperature</th><th>Probability Distribution</th><th>Behavioral Characteristics</th></tr></thead><tbody><tr><td>0</td><td>[1, 0, 0]</td><td>Completely deterministic (argmax)</td></tr><tr><td>1</td><td>[0.5, 0.3, 0.2]</td><td>Original model distribution</td></tr><tr><td>2</td><td>[0.41, 0.32, 0.27]</td><td>More smooth</td></tr><tr><td>5</td><td>[0.36, 0.33, 0.31]</td><td>Near-uniform distribution</td></tr></tbody></table>
<h3 id="key-distinction">Key Distinction</h3>
<ul>
<li><strong>Temperature</strong>: Changes probability distribution</li>
<li><strong>Sampling</strong>: Rolls the dice according to probability distribution</li>
</ul>
<p><code>temperature>0</code> doesn’t mean “will be random”—only when combined with sampling does it truly introduce randomness.</p>
<h2 id="summary">Summary</h2>
<p>The nondeterminism problem in LLM inference reveals a profound engineering truth:</p>
<blockquote>
<p><strong>A single forward pass is deterministic, but inference engines use different numerical computation paths under different loads for performance reasons.</strong></p>
</blockquote>
<p>The solution isn’t eliminating parallelism but <strong>freezing the parallel structure</strong> to keep the numerical path consistent in all cases. This is a clear engineering trade-off—exchanging some performance for complete determinism.</p>
<p>This solution is currently best suited for scenarios with extreme determinism requirements, particularly reinforcement learning training. It represents a new engineering perspective: sometimes, “slow but stable” is more valuable than “fast but erratic.”</p>
<hr>
<p><strong>References</strong>:</p>
<ul>
<li>Article: <a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/">Defeating Nondeterminism in LLM Inference</a></li>
<li>Code: <a href="https://github.com/thinking-machines-lab/batch_invariant_ops">batch_invariant_ops</a></li>
</ul>  </div> </article> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 Your name here. All rights reserved.
<div class="social-links" data-astro-cid-sz7xmlte> <a href="https://m.webtoo.ls/@astro" target="_blank" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>Follow Astro on Mastodon</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/mastodon" data-astro-cid-sz7xmlte><path fill="currentColor" d="M11.19 12.195c2.016-.24 3.77-1.475 3.99-2.603.348-1.778.32-4.339.32-4.339 0-3.47-2.286-4.488-2.286-4.488C12.062.238 10.083.017 8.027 0h-.05C5.92.017 3.942.238 2.79.765c0 0-2.285 1.017-2.285 4.488l-.002.662c-.004.64-.007 1.35.011 2.091.083 3.394.626 6.74 3.78 7.57 1.454.383 2.703.463 3.709.408 1.823-.1 2.847-.647 2.847-.647l-.06-1.317s-1.303.41-2.767.36c-1.45-.05-2.98-.156-3.215-1.928a3.614 3.614 0 0 1-.033-.496s1.424.346 3.228.428c1.103.05 2.137-.064 3.188-.189zm1.613-2.47H11.13v-4.08c0-.859-.364-1.295-1.091-1.295-.804 0-1.207.517-1.207 1.541v2.233H7.168V5.89c0-1.024-.403-1.541-1.207-1.541-.727 0-1.091.436-1.091 1.296v4.079H3.197V5.522c0-.859.22-1.541.66-2.046.456-.505 1.052-.764 1.793-.764.856 0 1.504.328 1.933.983L8 4.39l.417-.695c.429-.655 1.077-.983 1.934-.983.74 0 1.336.259 1.791.764.442.505.661 1.187.661 2.046v4.203z" data-astro-cid-sz7xmlte></path></svg> </a> <a href="https://twitter.com/astrodotbuild" target="_blank" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>Follow Astro on Twitter</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/twitter" data-astro-cid-sz7xmlte><path fill="currentColor" d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334 0-.14 0-.282-.006-.422A6.685 6.685 0 0 0 16 3.542a6.658 6.658 0 0 1-1.889.518 3.301 3.301 0 0 0 1.447-1.817 6.533 6.533 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.325 9.325 0 0 1-6.767-3.429 3.289 3.289 0 0 0 1.018 4.382A3.323 3.323 0 0 1 .64 6.575v.045a3.288 3.288 0 0 0 2.632 3.218 3.203 3.203 0 0 1-.865.115 3.23 3.23 0 0 1-.614-.057 3.283 3.283 0 0 0 3.067 2.277A6.588 6.588 0 0 1 .78 13.58a6.32 6.32 0 0 1-.78-.045A9.344 9.344 0 0 0 5.026 15z" data-astro-cid-sz7xmlte></path></svg> </a> <a href="https://github.com/withastro/astro" target="_blank" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>Go to Astro's GitHub repo</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/github" data-astro-cid-sz7xmlte><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-sz7xmlte></path></svg> </a> </div> </footer>  </body></html>